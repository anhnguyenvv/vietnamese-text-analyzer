import collections
import matplotlib.pyplot as plt
plt.switch_backend('Agg')  # Use non-GUI backend for matplotlib
from modules.preprocessing.preprocess import get_stopwords
from modules.preprocessing.normalization import normalize_text
from modules.preprocessing.tokenization import tokenize_sentences, tokenize_words
from wordcloud import WordCloud
import emoji

def create_wordcloud(word_freq):
    """
    T·∫°o bi·ªÉu ƒë·ªì WordCloud t·ª´ t·∫ßn su·∫•t t·ª´.
    Tr·∫£ v·ªÅ chu·ªói base64 c·ªßa ·∫£nh wordcloud.
    """
    import io, base64
    wordcloud = WordCloud(width=800, height=400, background_color='white', font_path='arial.ttf').generate_from_frequencies(word_freq)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout()
    buf = io.BytesIO()
    plt.savefig(buf, format='png')
    buf.seek(0)
    plot_data = base64.b64encode(buf.read()).decode('utf-8')
    buf.close()
    plt.close()
    return plot_data

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() == 'txt'

def create_plot(word_freq):
    top = word_freq.most_common(10)
    words, freqs = zip(*top)
    plt.figure(figsize=(8, 4))
    plt.bar(words, freqs, color='teal')
    plt.xticks(rotation=45)
    plt.title("Top 10 t·ª´ ph·ªï bi·∫øn")
    plt.tight_layout()
    import io, base64
    buf = io.BytesIO()
    plt.savefig(buf, format='png')
    buf.seek(0)
    plot_data = base64.b64encode(buf.read()).decode('utf-8')
    buf.close()
    plt.close()
    return plot_data

def analyze_text(text, remove_stopwords=True):    
    clean_text = normalize_text(text, remove_icon=True)
    sentences = tokenize_sentences(clean_text) 
    words = tokenize_words(clean_text)
    num_digits = sum(c.isdigit() for c in clean_text) 
    # x√≥a s·ªë v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
    words = [w for w in words if w.isalpha()]
    chars = len(text)
    
    stopwords = set(get_stopwords())
    filtered_words = [w for w in words if w.lower() not in stopwords] 
    stopword_count =  len(words) - len(filtered_words)
    word_freq = collections.Counter([w for w in (filtered_words if remove_stopwords else words)])
    result = {
        "num_sentences": len(sentences),
        "num_words": len(filtered_words),
        "num_chars": chars,
        "avg_sentence_len": round(sum(len(tokenize_words(s)) for s in sentences) / len(sentences), 2) if sentences else 0,
        "avg_word_len": round(sum(len(w) for w in filtered_words) / len(filtered_words), 2) if filtered_words else 0,
        "vocab_size": len(set(filtered_words)),
        "num_digits": num_digits,
        "num_special_chars": sum(1 for c in text if not c.isalnum() and not c.isspace()),
        "num_emojis": sum(1 for c in text if emoji.is_emoji(c)),
        "num_stopwords": stopword_count,
        "word_freq": word_freq,
    }
    return result

def analyze_file(file_path, remove_stopwords=False):
    if not allowed_file(file_path):
        raise ValueError("Ch·ªâ h·ªó tr·ª£ file .txt")

    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    return analyze_text(text, remove_stopwords=remove_stopwords)
    # ph√¢n t√≠ch file csv
    # if file_path.endswith('.csv'):
    #     import pandas as pd
    #     df = pd.read_csv(file_path)
    #     results = []
    #     for col in df.columns:
    #         if df[col].dtype == 'object':  # Ch·ªâ ph√¢n t√≠ch c
    #             text = ' '.join(df[col].dropna().astype(str).tolist())
    #             result = analyze_text(text, remove_stopwords=remove_stopwords)
    #             results.append({
    #                 "column": col,
    #                 "analysis": result
    #             })
    #     return results
if __name__ == '__main__':
    # Example usage
    text = '''ELO 3. Ng·ªØ c·∫£nh, tr√°ch nhi·ªám v√† ƒë·∫°o ƒë·ª©c 
ELO 3. 1. Ng·ªØ c·∫£nh b√™n ngo√†i, x√£ h·ªôi, kinh t·∫ø v√† m√¥i tr∆∞·ªùng 
ELO 3. 1. 1 C√°c v·∫•n ƒë·ªÅ v√† gi√° tr·ªã c·ªßa x√£ h·ªôi, kinh t·∫ø v√† m√¥i tr∆∞·ªùng ƒë∆∞∆°ng ƒë·∫°i 
ELO 3. 1. 2 Vai tr√≤ v√† tr√°ch nhi·ªám 
ELO 3. 1. 3 Ng·ªØ c·∫£nh vƒÉn h√≥a, l·ªãch s·ª≠ 
ELO 3. 1. 4 Lu·∫≠t l·ªá v√† quy ƒë·ªãnh c·ªßa x√£ h·ªôi 
ELO 3. 2. Ng·ªØ c·∫£nh c√¥ng ty v√† doanh nghi·ªáp 
ELO 3. 2. 1 Ng·ªØ c·∫£nh v√† vƒÉn h√≥a c·ªßa c√¥ng ty, t·ªï ch·ª©c 
ELO 3. 2. 2 C√°c b√™n li√™n quan, m·ª•c ti√™u v√† chi·∫øn l∆∞·ª£c c·ªßa c√¥ng ty/ doanh nghi·ªáp 
ELO 3. 2. 3 Lu·∫≠t l·ªá v√† quy ƒë·ªãnh c·ªßa c√¥ng ty/ doanh nghi·ªáp 
ELO 3. 3. ƒê·∫°o ƒë·ª©c, tr√°ch nhi·ªám v√† c√°c gi√° tr·ªã c√° nh√¢n c·ªët l√µi 
ELO 3. 3. 1 C√°c chu·∫©n m·ª±c v√† nguy√™n t·∫Øc ƒë·∫°o ƒë·ª©c 
ELO 3. 3. 2 Tr√°ch nhi·ªám v√† c√°ch h√†nh x·ª≠ chuy√™n nghi·ªáp 
ELO 3. 3. 3 S·ª± cam k·∫øt 
ELO 3. 3. 4 Trung th·ª±c, uy t√≠n v√† trung th√†nh üßêüòó‚ò∫Ô∏è'''
    result = analyze_text(text, remove_stopwords=True)
    print(result)
    print(f'top 10 t·ª´: {result["word_freq"].most_common(10)}')
    plot_url = create_plot(result['word_freq'])
    print(f"Plot URL: {plot_url[:50]}...")  # Print first 50 characters of the plot URL
    wordcloud_url = create_wordcloud(result['word_freq'])
    print(f"Wordcloud URL: {wordcloud_url[:50]}...")  # Print first 50 characters of the wordcloud URL